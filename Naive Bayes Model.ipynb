{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Video Classifier\n",
    "This notebook runs through YouTube Trending Video the classification of YouTube Trending video category based on title using Naive Bayes.\n",
    "\n",
    "Additionally, we will compare the performance of two specific variants of Navive Bayes\n",
    "    1. Multinomial Naive Bayes\n",
    "    2. Bernouli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea610d38fd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationReport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yellowbrick'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from yellowbrick.classifier import ClassificationReport, ConfusionMatrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in filtered dataset and look at the formatting\n",
    "The columns of interest at ['filter_title', 'filter_title_no_stops', 'category_id']\n",
    "    1. 'filter_title' --> raw input that only has punctuation removed and letters to lower case\n",
    "    2. 'filter_title_no_stops' --> 'filter_title' data with addition to removal of stopwords\n",
    "    3. 'category_id' --> output/buckets to categorize video (16 total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles_info = pd.read_csv('./output/US_count_vectorizer_dataset.csv')\n",
    "df_titles_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the dataframe into inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_titles_info['filter_title']\n",
    "df_y = df_titles_info['category_id']\n",
    "df_x_stop = df_titles_info['filter_title_no_stops']\n",
    "\n",
    "target_names = list(df_titles_info['category_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset before vectorizing\n",
    "This guards against leaking information from testing to training set (80% training, 20% testing)\n",
    "\n",
    "https://machinelearningmastery.com/data-leakage-machine-learning/\n",
    "https://stackoverflow.com/questions/54491953/can-i-use-countvectorizer-on-both-test-and-train-data-at-the-same-time-or-do-i-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=117)\n",
    "x_train_stop, x_test_stop = train_test_split(df_x_stop, test_size=0.2, random_state=117)\n",
    "\n",
    "x_train_stop = x_train_stop.fillna(' ')\n",
    "x_test_stop = x_test_stop.fillna(' ')\n",
    "print(\"Training data size:\", x_train.shape)\n",
    "print(\"Testing data size:\", x_test.shape)\n",
    "print(\"Training data size:\", x_train_stop.shape)\n",
    "print(\"Testing data size:\", x_test_stop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer words using CountVectorizer\n",
    "Bag-of-Words model that allows both tokenize a collection of text documents and build a vocabulary of known words\n",
    "\n",
    "The length of each individual vector will be that of the entire dataset which each indices representing the count of a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_count_vector = count_vectorizer.fit_transform(x_train)\n",
    "test_count_vector = count_vectorizer.transform(x_test)\n",
    "\n",
    "train_count_vector_stop = count_vectorizer.fit_transform(x_train_stop)\n",
    "test_count_vector_stop = count_vectorizer.transform(x_test_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer words using TfidfTransformer\n",
    "Will convert our count values from CountVectorizer into a frequency matrix\n",
    "\n",
    "Term Frequency: How often a word appears in a particular title.\n",
    "Inverse Document Frequency: Downscale this words that appear often across multiple titles.\n",
    "\n",
    "Main purpose is to reduce the importance of stopwords that a common accross categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfTransformer()\n",
    "x_trained_tfidf_vector = tfidf_vectorizer.fit_transform(train_count_vector)\n",
    "x_test_tfidf_vector = tfidf_vectorizer.transform(test_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model Training\n",
    "\n",
    "Create Naive Bayes Model on 3 inputs from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_count = MultinomialNB()\n",
    "clf_count.fit(train_count_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_count_stop = MultinomialNB()\n",
    "clf_count_stop.fit(train_count_vector_stop, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(x_trained_tfidf_vector, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Classification Report\n",
    "Test the accuracy of our model usingth testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tfidf = clf_tfidf.predict(x_test_tfidf_vector)\n",
    "print(\"Tfidf Model\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test, pred_tfidf))\n",
    "print(\"Precision: \", metrics.precision_score(y_test, pred_tfidf, average='weighted'))\n",
    "print(\"F1: \", metrics.f1_score(y_test, pred_tfidf, average='weighted'))\n",
    "print(metrics.classification_report(y_test, pred_tfidf))\n",
    "\n",
    "\n",
    "visualizer = ClassificationReport(clf_tfidf, support=True, cmap='Greens')\n",
    "visualizer.fit(x_trained_tfidf_vector, y_train)\n",
    "visualizer.score(x_test_tfidf_vector, y_test)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count = clf_count.predict(test_count_vector)\n",
    "print(\"Count Model\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test, pred_count))\n",
    "print(\"Precision: \", metrics.precision_score(y_test, pred_count, average='weighted'))\n",
    "print(\"F1: \", metrics.f1_score(y_test, pred_count, average='weighted'))\n",
    "print(metrics.classification_report(y_test, pred_count))\n",
    "\n",
    "\n",
    "plt.title(\"Testing\")\n",
    "visualizer = ClassificationReport(clf_count, support=True, cmap='Greens')\n",
    "visualizer.fit(train_count_vector, y_train)\n",
    "visualizer.score(test_count_vector, y_test)\n",
    "visualizer.finalize(set_title=\"this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count_stop = clf_count_stop.predict(test_count_vector_stop)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, pred_count_stop))\n",
    "print(\"Precision: \", metrics.precision_score(y_test, pred_count_stop, average='weighted'))\n",
    "print(\"F1: \", metrics.f1_score(y_test, pred_count_stop, average='weighted'))\n",
    "print(metrics.classification_report(y_test, pred_count_stop))\n",
    "\n",
    "\n",
    "visualizer = ClassificationReport(clf_count_stop, support=True, cmap='Greens')\n",
    "visualizer.fit(train_count_vector_stop, y_train)\n",
    "visualizer.score(test_count_vector_stop, y_test)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Graphical representation of accuracy in terms of False Negative/False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = ConfusionMatrix(clf_count, percent=True, cmap='Greens')\n",
    "conf_matrix.fit(train_count_vector, y_train)\n",
    "conf_matrix.score(test_count_vector, y_test)\n",
    "conf_matrix.show()\n",
    "\n",
    "conf_matrix = ConfusionMatrix(clf_tfidf, percent=True, cmap='Greens')\n",
    "conf_matrix.fit(x_trained_tfidf_vector, y_train)\n",
    "conf_matrix.score(x_test_tfidf_vector, y_test)\n",
    "conf_matrix.show()\n",
    "\n",
    "conf_matrix = ConfusionMatrix(clf_count_stop, percent=True, cmap='Greens')\n",
    "conf_matrix.fit(train_count_vector_stop, y_train)\n",
    "conf_matrix.score(test_count_vector_stop, y_test)\n",
    "conf_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparision between model variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_accuracy = accuracy_score(y_test, pred_tfidf)\n",
    "regular_accuracy = accuracy_score(y_test, pred_count)\n",
    "regular_accuracy_stop = accuracy_score(y_test, pred_count_stop)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "plt.ylim(0, 1)\n",
    "x = [\"Regular Filtered\", \"Filtered/No Stopwords\", \"Filterd/Normalized\"]\n",
    "y = [regular_accuracy, regular_accuracy_stop, normalized_accuracy]\n",
    "\n",
    "fig.suptitle('LogReg Input vs Accuracy', fontsize=15)\n",
    "plt.xlabel('Input Type', fontsize=15)\n",
    "plt.ylabel('Accuracy Percentage', fontsize=15)\n",
    "ax.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Overall, our title classification using Naive Bayes seems to perform well with accuracy ranging between 86-90%. When looking at the titles of YouTube videos, we often see that each category are highly seperable. For example, videos about Sports or Education often will use very specific words that are exclusive to that category. However, our model does seem to struggle when classifing Entertainment videos as the title wordings seems to be more generalized. \"the trump presidency last week tonight with jo...\" is under the Entertainment category when wordings suggests it could be part of News & Politics instead.\n",
    "\n",
    "We are confident in the performance of our Naive Bayes Classifer model due to the highly seperable categories on the input dataset and our methods of data separation to ensure no data leakage occurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
